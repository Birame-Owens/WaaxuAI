# -*- coding: utf-8 -*-
"""WaaxuAI_PoC_v2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gcZyD0GJiCrDGcS39qkVZFiIklvPm815

# ğŸŒ WaaxuAI â€” Proof of Concept
## DÃ©tection Multilingue de Discours Haineux
### iSAFE Hackathon 2026 | WSIS Forum

---

**ModÃ¨le :** XLM-RoBERTa (multilingue, 100+ langues)  
**Dataset :** Hate Speech and Offensive Language Dataset  
**Objectif :** Classifier les textes en 3 catÃ©gories : ğŸ”´ HAINEUX / ğŸŸ¡ OFFENSANT / ğŸŸ¢ NORMAL

---

## âš™ï¸ Ã‰TAPE 1 â€” Installation des librairies
"""

!pip install transformers scikit-learn matplotlib seaborn torch -q
print('âœ… Librairies installÃ©es avec succÃ¨s !')

"""## ğŸ“¦ Ã‰TAPE 2 â€” Chargement du Dataset

> **Instructions :**  
> Quand la cellule s'exÃ©cute, une fenÃªtre **"Choisir un fichier"** va s'ouvrir.  
> Navigue vers `C:\Users\biram\Downloads\labeled_data.csv` et sÃ©lectionne-le.
"""

from google.colab import files
import pandas as pd

print('ğŸ“ SÃ©lectionne ton fichier labeled_data.csv...')
uploaded = files.upload()

df = pd.read_csv('labeled_data.csv')
print('âœ… Dataset chargÃ© avec succÃ¨s !')
print(f'ğŸ“Š Shape : {df.shape}')
print(f'ğŸ“‹ Colonnes : {df.columns.tolist()}')
df.head()

"""## ğŸ” Ã‰TAPE 3 â€” Exploration & PrÃ©paration des donnÃ©es"""

import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# Nettoyage et mapping des labels
df = df[['tweet', 'class']].dropna()
df.columns = ['text', 'label']

# 0 = Hate Speech, 1 = Offensive, 2 = Normal
label_names  = {0: 'ğŸ”´ HAINEUX', 1: 'ğŸŸ¡ OFFENSANT', 2: 'ğŸŸ¢ NORMAL'}
label_colors = {0: '#e74c3c',    1: '#f39c12',       2: '#2ecc71'}

print('âœ… Distribution des classes :')
print(df['label'].value_counts())
print(f'\nğŸ“ Total exemples : {len(df)}')

# ğŸ“Š Graphique 1 : Distribution des classes
fig, axes = plt.subplots(1, 2, figsize=(14, 5))
fig.suptitle('WaaxuAI â€” Distribution du Dataset', fontsize=16, fontweight='bold', color='#1F4E79')

counts = df['label'].value_counts().sort_index()
colors          = [label_colors[i] for i in counts.index]
labels_display  = [label_names[i]  for i in counts.index]

# Bar chart
bars = axes[0].bar(labels_display, counts.values, color=colors, edgecolor='white', linewidth=1.5)
axes[0].set_title('Nombre d\'exemples par classe', fontweight='bold')
axes[0].set_ylabel('Nombre de tweets')
for bar, val in zip(bars, counts.values):
    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50,
                 f'{val:,}', ha='center', fontweight='bold')

# Pie chart
axes[1].pie(counts.values, labels=labels_display, colors=colors,
            autopct='%1.1f%%', startangle=90,
            wedgeprops={'edgecolor': 'white', 'linewidth': 2})
axes[1].set_title('RÃ©partition en pourcentage', fontweight='bold')

plt.tight_layout()
plt.savefig('distribution.png', dpi=150, bbox_inches='tight')
plt.show()
print('âœ… Graphique sauvegardÃ© !')

import re
from sklearn.model_selection import train_test_split

# Nettoyage du texte
def clean_text(text):
    text = re.sub(r'http\S+|www\S+', '', str(text))  # Supprimer URLs
    text = re.sub(r'@\w+', '', text)                  # Supprimer mentions
    text = re.sub(r'#', '', text)                     # Nettoyer hashtags
    text = re.sub(r'\s+', ' ', text).strip()          # Espaces multiples
    return text

df['text'] = df['text'].apply(clean_text)

# Sous-ensemble Ã©quilibrÃ© pour aller plus vite sur Colab
df_sample = df.groupby('label').apply(
    lambda x: x.sample(min(1500, len(x)), random_state=42)
).reset_index(drop=True)

# Split train / test
X_train, X_test, y_train, y_test = train_test_split(
    df_sample['text'].tolist(),
    df_sample['label'].tolist(),
    test_size=0.2,
    random_state=42,
    stratify=df_sample['label']
)

print(f'âœ… DonnÃ©es prÃªtes !')
print(f'   Train : {len(X_train)} exemples')
print(f'   Test  : {len(X_test)} exemples')

"""## ğŸ¤– Ã‰TAPE 4 â€” Chargement du modÃ¨le XLM-RoBERTa"""

import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from transformers import get_linear_schedule_with_warmup

# VÃ©rification GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'ğŸ–¥ï¸  Device utilisÃ© : {device}')
if torch.cuda.is_available():
    print(f'ğŸš€ GPU : {torch.cuda.get_device_name(0)}')
else:
    print('âš ï¸  Pas de GPU dÃ©tectÃ© â€” Va dans ExÃ©cution â†’ Modifier le type d\'exÃ©cution â†’ T4 GPU')

# Chargement tokenizer et modÃ¨le
MODEL_NAME = 'xlm-roberta-base'
NUM_LABELS = 3

print(f'\nâ³ Chargement de {MODEL_NAME}...')
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=NUM_LABELS)
model = model.to(device)
print(f'âœ… ModÃ¨le chargÃ© ! ParamÃ¨tres : {sum(p.numel() for p in model.parameters()):,}')

# Dataset PyTorch
class HateSpeechDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len=128):
        self.texts     = texts
        self.labels    = labels
        self.tokenizer = tokenizer
        self.max_len   = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        encoding = self.tokenizer(
            self.texts[idx],
            max_length=self.max_len,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        return {
            'input_ids':      encoding['input_ids'].squeeze(),
            'attention_mask': encoding['attention_mask'].squeeze(),
            'label':          torch.tensor(self.labels[idx], dtype=torch.long)
        }

# DataLoaders
BATCH_SIZE = 16

train_dataset = HateSpeechDataset(X_train, y_train, tokenizer)
test_dataset  = HateSpeechDataset(X_test,  y_test,  tokenizer)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False)

print(f'âœ… DataLoaders prÃªts !')
print(f'   Train batches : {len(train_loader)}')
print(f'   Test batches  : {len(test_loader)}')

"""## ğŸ‹ï¸ Ã‰TAPE 5 â€” EntraÃ®nement du modÃ¨le"""

from sklearn.metrics import accuracy_score

# HyperparamÃ¨tres
EPOCHS = 3
LR     = 2e-5

optimizer     = AdamW(model.parameters(), lr=LR, weight_decay=0.01)
total_steps   = len(train_loader) * EPOCHS
scheduler     = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=total_steps // 10,
    num_training_steps=total_steps
)

train_losses, val_accuracies = [], []

print('ğŸš€ DÃ©but de l\'entraÃ®nement...\n')
print('â±ï¸  DurÃ©e estimÃ©e : ~15-20 min avec GPU T4\n')

for epoch in range(EPOCHS):

    # â”€â”€ Training
    model.train()
    total_loss = 0

    for i, batch in enumerate(train_loader):
        input_ids      = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels         = batch['label'].to(device)

        optimizer.zero_grad()
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss    = outputs.loss
        total_loss += loss.item()

        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        scheduler.step()

        if (i + 1) % 20 == 0:
            print(f'  Epoch {epoch+1}/{EPOCHS} | Batch {i+1}/{len(train_loader)} | Loss : {loss.item():.4f}')

    avg_loss = total_loss / len(train_loader)
    train_losses.append(avg_loss)

    # â”€â”€ Validation
    model.eval()
    all_preds, all_labels = [], []

    with torch.no_grad():
        for batch in test_loader:
            input_ids      = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels         = batch['label'].to(device)
            outputs        = model(input_ids=input_ids, attention_mask=attention_mask)
            preds          = torch.argmax(outputs.logits, dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    acc = accuracy_score(all_labels, all_preds)
    val_accuracies.append(acc)
    print(f'\nğŸ“Š Epoch {epoch+1}/{EPOCHS} â€” Loss : {avg_loss:.4f} | Accuracy : {acc*100:.2f}%\n')

print('âœ… EntraÃ®nement terminÃ© !')

"""## ğŸ“Š Ã‰TAPE 6 â€” Ã‰valuation & Graphiques des performances"""

# Graphique 2 : Courbes d'entraÃ®nement
fig, axes = plt.subplots(1, 2, figsize=(14, 5))
fig.suptitle('WaaxuAI â€” Courbes d\'EntraÃ®nement', fontsize=16, fontweight='bold', color='#1F4E79')

epochs_range = range(1, EPOCHS + 1)

axes[0].plot(epochs_range, train_losses, 'o-', color='#e74c3c', linewidth=2, markersize=8)
axes[0].set_title('Loss par Epoch', fontweight='bold')
axes[0].set_xlabel('Epoch')
axes[0].set_ylabel('Loss')
axes[0].grid(True, alpha=0.3)
for i, v in enumerate(train_losses):
    axes[0].annotate(f'{v:.4f}', (i+1, v), textcoords='offset points', xytext=(0,10), ha='center')

axes[1].plot(epochs_range, [a*100 for a in val_accuracies], 'o-', color='#2ecc71', linewidth=2, markersize=8)
axes[1].set_title('Accuracy par Epoch', fontweight='bold')
axes[1].set_xlabel('Epoch')
axes[1].set_ylabel('Accuracy (%)')
axes[1].set_ylim([0, 100])
axes[1].grid(True, alpha=0.3)
for i, v in enumerate(val_accuracies):
    axes[1].annotate(f'{v*100:.1f}%', (i+1, v*100), textcoords='offset points', xytext=(0,10), ha='center')

plt.tight_layout()
plt.savefig('training_curves.png', dpi=150, bbox_inches='tight')
plt.show()
print('âœ… Courbes sauvegardÃ©es !')

from sklearn.metrics import classification_report, confusion_matrix
import numpy as np

class_names = ['HAINEUX', 'OFFENSANT', 'NORMAL']

# Rapport de classification
print('=' * 60)
print('        WaaxuAI â€” RAPPORT DE CLASSIFICATION')
print('=' * 60)
print(classification_report(all_labels, all_preds, target_names=class_names))

# Graphique 3 : Matrice de confusion
cm = confusion_matrix(all_labels, all_preds)

plt.figure(figsize=(8, 6))
sns.heatmap(
    cm, annot=True, fmt='d', cmap='Blues',
    xticklabels=class_names, yticklabels=class_names,
    linewidths=1, linecolor='white', annot_kws={'size': 14, 'weight': 'bold'}
)
plt.title('WaaxuAI â€” Matrice de Confusion', fontsize=14, fontweight='bold', color='#1F4E79', pad=15)
plt.ylabel('Vraie classe', fontweight='bold')
plt.xlabel('Classe prÃ©dite', fontweight='bold')
plt.tight_layout()
plt.savefig('confusion_matrix.png', dpi=150, bbox_inches='tight')
plt.show()

print(f'\nğŸ¯ Accuracy finale : {val_accuracies[-1]*100:.2f}%')

"""## ğŸ§ª Ã‰TAPE 7 â€” Test en temps rÃ©el avec tes propres phrases"""

import torch.nn.functional as F

def predict_waaxuai(text):
    """PrÃ©dit la classe d'un texte avec WaaxuAI"""
    model.eval()
    encoding = tokenizer(
        text,
        max_length=128,
        padding='max_length',
        truncation=True,
        return_tensors='pt'
    )
    input_ids      = encoding['input_ids'].to(device)
    attention_mask = encoding['attention_mask'].to(device)

    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        probs   = F.softmax(outputs.logits, dim=1).squeeze().cpu().numpy()

    pred  = probs.argmax()
    icons = ['ğŸ”´ HAINEUX', 'ğŸŸ¡ OFFENSANT', 'ğŸŸ¢ NORMAL']

    print(f'\nğŸ“ Texte     : "{text}"')
    print(f'ğŸ¯ RÃ©sultat  : {icons[pred]} (confiance : {probs[pred]*100:.1f}%)')
    print(f'   Scores   â†’ Haineux : {probs[0]*100:.1f}% | Offensant : {probs[1]*100:.1f}% | Normal : {probs[2]*100:.1f}%')
    print('-' * 70)
    return pred, probs

# â”€â”€ Tests avec phrases variÃ©es (multilingues !)
print('=' * 70)
print('           ğŸŒ WaaxuAI â€” TEST EN TEMPS RÃ‰EL')
print('=' * 70)

test_phrases = [
    # Anglais
    "I hate all people from that country, they should be banned!",
    "This is a great day, I love this community!",
    # FranÃ§ais
    "Ce groupe de personnes est dangereux pour la sociÃ©tÃ©.",
    "Nous devons travailler ensemble pour un monde meilleur.",
    # Wolof
    "Dafa neex ak yÃ«gÃ«l ci biir internet bi.",
    # Phrase neutre
    "The weather today is quite nice outside."
]

for phrase in test_phrases:
    predict_waaxuai(phrase)

# â”€â”€ Teste avec TA PROPRE phrase !
print('âœï¸  Entre ta propre phrase Ã  analyser :')
ma_phrase = input('>>> ')
predict_waaxuai(ma_phrase)

"""## ğŸ’¾ Ã‰TAPE 8 â€” Sauvegarde du modÃ¨le"""

# Sauvegarde locale dans Colab
model.save_pretrained('./waaxuai_model')
tokenizer.save_pretrained('./waaxuai_model')
print('âœ… ModÃ¨le sauvegardÃ© dans ./waaxuai_model')

# â”€â”€ Optionnel : Sauvegarder sur Google Drive
# from google.colab import drive
# drive.mount('/content/drive')
# !cp -r ./waaxuai_model /content/drive/MyDrive/WaaxuAI/
# print('âœ… ModÃ¨le sauvegardÃ© sur Google Drive !')

print('\n' + '=' * 60)
print('       ğŸ‰ WaaxuAI PoC terminÃ© avec succÃ¨s !')
print('=' * 60)
print(f'  ğŸ¯ Accuracy finale  : {val_accuracies[-1]*100:.2f}%')
print(f'  ğŸŒ Langues couvertes : 100+ (XLM-RoBERTa)')
print(f'  ğŸ“¦ ModÃ¨le           : xlm-roberta-base fine-tunÃ©')
print(f'  ğŸ† PrÃªt pour        : iSAFE Hackathon SMSI 2026')
print('=' * 60)

"""---

## ğŸ“‹ RÃ©sumÃ© du PoC

| Ã‰lÃ©ment | DÃ©tail |
|---------|--------|
| **ModÃ¨le** | XLM-RoBERTa Base (multilingue, 100+ langues) |
| **Dataset** | Hate Speech & Offensive Language Dataset |
| **TÃ¢che** | Classification 3 classes : Haineux / Offensant / Normal |
| **Framework** | PyTorch + Hugging Face Transformers |
| **Langues testÃ©es** | Anglais, FranÃ§ais, Wolof |
| **Graphiques** | Distribution, Courbes d'entraÃ®nement, Matrice de confusion |

---

*WaaxuAI â€” Building Digital Peace, One Language at a Time ğŸŒ*  
*iSAFE Hackathon 2026 | WSIS Forum*
"""